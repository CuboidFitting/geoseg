{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Geoseg \n",
    "> __version__ ==   0.1.0\n",
    "\n",
    ">  __author__ ==  Go-Hiroaki\n",
    "\n",
    "# Overview:\n",
    "\n",
    "## 1. Evaluating with pretrained models\n",
    "> Test model performance by providing pretrained models\n",
    "\n",
    "## 2. Re-training with provided dataset\n",
    "> Trained new models with provide training datastet\n",
    "\n",
    "## 3. Training with personal dataset\n",
    "> Train and test models with your own dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How-to.ipynb  __init__.py  \u001b[0m\u001b[01;34mdataset\u001b[0m/  \u001b[01;34mlogs\u001b[0m/    \u001b[01;34msrc\u001b[0m/\r\n",
      "LICENSE       \u001b[01;34mcheckpoint\u001b[0m/  eva.sh    \u001b[01;34mresult\u001b[0m/  visSingle.py\r\n",
      "README.md     \u001b[01;34mdata\u001b[0m/        \u001b[01;34mexample\u001b[0m/  run.sh   visSingleComparison.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluating with pretrained models\n",
    "\n",
    "### 1.1 Prepared and loaded dataset\n",
    "#### > Prepared  dataset\n",
    "\n",
    "```\n",
    "YOUR_DATASET/\n",
    "|-- img\n",
    "|   |-- train_1.png\n",
    "|   |-- train_2.png\n",
    "|   `-- \n",
    "|-- msk\n",
    "|   |-- train_1.png\n",
    "|   |-- train_2.png\n",
    "|   `-- \n",
    "|-- ref.csv\n",
    "|-- statistic.csv\n",
    "|-- train.csv\n",
    "|-- val.csv\n",
    "```\n",
    "\n",
    "#### > Modified src/datasets.py to make sure YOUR_DATASET \n",
    "\n",
    "```\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ====================== parameter initialization ======================= #\n",
    "    parser = argparse.ArgumentParser(description='ArgumentParser')\n",
    "    parser.add_argument('-idx', type=int, default=0,\n",
    "                        help='index of sample image')\n",
    "    args = parser.parse_args()\n",
    "    idx = args.idx\n",
    "    for root in ['YOUR_DATASET']:\n",
    "        for mode in [\"IM\", \"IMS\", \"IME\"]:\n",
    "            print(\"Load {}/{}.\".format(root, mode))\n",
    "            trainset, valset = load_dataset(root, mode)\n",
    "            \n",
    "            # print(\"Load train set = {} examples, val set = {} examples\".format(\n",
    "            #     len(trainset), len(valset)))\n",
    "            sample = trainset[idx]\n",
    "            trainset.show(idx)\n",
    "            sample = valset[idx]\n",
    "            valset.show(idx)\n",
    "            print(\"\\tsrc:\", sample[\"src\"].shape,\n",
    "                  \"tar:\", sample[\"tar\"].shape,)\n",
    "\n",
    "```\n",
    "#### > Run src/datasets.py\n",
    "> python src/datasets.py\n",
    "\n",
    "if success, sample image will show up in example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download pretrained models\n",
    "\n",
    "> 1. FCN8s_iter_5000.pth [LINK](https://drive.google.com/open?id=1KHs7coyXAipz8t5cN_lbTC4MOYi8FddI)\n",
    "> 2. FCN16s_iter_5000.pth [LINK](https://drive.google.com/open?id=1wlORkMx_ykmHysShUKY4UcCYs-fVaen6)\n",
    "> 3. FCN32s_iter_5000.pth [LINK](https://drive.google.com/open?id=1OR_Sk66RAGtKrp0quvqazRkL0xtAH8RY)\n",
    "> 4. SegNet_iter_5000.pth [LINK](https://drive.google.com/open?id=1J0aRjFG-zOSSXnynm02VaYxjw1tjx-qC)\n",
    "> 5. UNet_iter_5000.pth [LINK](https://drive.google.com/open?id=17X0aCgRx3XXgH1fcfLoLwgcbWIzxZe5K)\n",
    "> 6. FPN_iter_5000.pth [LINK](https://drive.google.com/open?id=1fWrCnGQJBZTw7m5OZlQvH5-R_JJlBA-r)\n",
    "> 7. ResUNet_iter_5000.pth [LINK](https://drive.google.com/open?id=1jGs_PxEMXCshOzXdg9LuFJxe8kO39oxT)\n",
    "> 8. MC-FCN_iter_5000.pth [LINK](https://drive.google.com/open?id=1Kt_JmR0ZGXvK9kuTmDOek5l1SsHX4xhz)\n",
    "> 9. BR-Net_iter_5000.pth [LINK](https://drive.google.com/open?id=1rytD9tzAq2mne5yf3XEh-jTSHlvQvedT)\n",
    "> * Upcoming ...\n",
    "\n",
    "    After downloading corresponding pretrained models, save them at checkpoints/ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRNet-3*1*24-NZ32km2_iter_5000.pth      FPN-3*1*24-NZ32km2_iter_5000.pth\r\n",
      "BRNet-3*6*24-Vaihingen_iter_5000.pth    FPN-3*6*24-Vaihingen_iter_5000.pth\r\n",
      "FCN16s-3*1*24-NZ32km2_iter_5000.pth     MCFCN-3*1*24-NZ32km2_iter_5000.pth\r\n",
      "FCN16s-3*6*24-Vaihingen_iter_5000.pth   MCFCN-3*6*24-Vaihingen_iter_5000.pth\r\n",
      "FCN32s-3*1*24-NZ32km2_iter_5000.pth     SegNet-3*1*24-NZ32km2_iter_5000.pth\r\n",
      "FCN32s-3*6*24-PotsdamRGB_iter_5000.pth  SegNet-3*6*24-Vaihingen_iter_5000.pth\r\n",
      "FCN32s-3*6*24-Vaihingen_iter_5000.pth   UNet-3*1*24-NZ32km2_iter_5000.pth\r\n",
      "FCN8s-3*1*24-NZ32km2_iter_5000.pth      UNet-3*6*24-Vaihingen_iter_5000.pth\r\n",
      "FCN8s-3*6*24-Vaihingen_iter_5000.pth    checkpoint.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls ./checkpoint/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Run evaluation scripts\n",
    "\n",
    "* sinle model\n",
    "\n",
    "```\n",
    "visSingle.py -h\n",
    "   optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -checkpoints CHECKPOINTS [CHECKPOINTS ...]\n",
    "                        checkpoints used for making prediction\n",
    "  -spaces SPACES [SPACES ...]\n",
    "                        barrier space for merging\n",
    "  -direction {horizontal,vertical}\n",
    "                        merge image direction\n",
    "  -disp_cols DISP_COLS  cols for displaying image\n",
    "  -edge_fn {shift,canny}\n",
    "                        method used for edge extraction\n",
    "  -gen_nb GEN_NB        number of generated image\n",
    "  -color COLOR          background color for generated rgb result\n",
    "  -partition PARTITION  partition of dataset for loading\n",
    "  -disk DISK            dilation level\n",
    "  -cuda CUDA            using cuda for optimization\n",
    "```\n",
    "  \n",
    "  The generate result will show up at result/single\n",
    "   - BR-Net ![time](./result/single/BR-Net_canny_segmap_edge_0.png)\n",
    "   \n",
    "* multi models\n",
    "```\n",
    "visSingleComparison.py -h\n",
    "    optional arguments:\n",
    "    -h, --help            show this help message and exit\n",
    "    -checkpoints CHECKPOINTS [CHECKPOINTS ...]\n",
    "                        checkpoints used for making prediction\n",
    "    -spaces SPACES [SPACES ...]\n",
    "                        barrier spaces for merging\n",
    "    -direction {horizontal,vertical}\n",
    "                        merge image direction\n",
    "    -disp_cols DISP_COLS  cols for displaying image\n",
    "    -target {segmap,edge}\n",
    "                        target for model prediction [segmap, edge]\n",
    "    -edge_fn {shift,canny}\n",
    "                        method used for edge extraction\n",
    "    -gen_nb GEN_NB        number of generated image\n",
    "    -eval_fn {ov,precision,recall,f1_score,jaccard,kappa}\n",
    "                        method used for evaluate performance\n",
    "    -significance SIGNIFICANCE\n",
    "                        significant different level between methods\n",
    "    -color COLOR          background color for generated rgb result\n",
    "    -partition PARTITION  partition of dataset for loading\n",
    "    -disk DISK            dilation level\n",
    "    -batch_size BATCH_SIZE\n",
    "                        batch size for model prediction\n",
    "    -cuda CUDA            using cuda for optimization\n",
    "```\n",
    "  The generate result will show up at result/single-comparison\n",
    "   - Segmap FCN32s_FCN16s_FCN8s ![time](./result/single-comparison/segmap_FCN32s_FCN16s_FCN8s_1.png)\n",
    "   - Edge FCN32s_FCN16s_FCN8s ![time](./result/single-comparison/edge_FCN32s_FCN16s_FCN8s_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Re-train with provided dataset\n",
    "\n",
    "### 2.1 Download training dataset\n",
    "> Training dataset [LINK](https://drive.google.com/file/d/1boGcJz9TyK9XB4GUhjCHVu8XGtbgjjbi/view?usp=sharing).\n",
    "Unzip and place to datasets/\n",
    "\n",
    "### 2.2 Run training scripts\n",
    "\n",
    "```\n",
    "python src/train.py -h\n",
    "usage: train.py [-h] [-root ROOT] [-net NET] [-base_kernel BASE_KERNEL] [-trigger {epoch,iter}] [-interval INTERVAL]\n",
    "                [-terminal TERMINAL] [-batch_size BATCH_SIZE] [-lr LR] [-cuda CUDA]\n",
    "\n",
    "ArgumentParser\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -root ROOT            root dir of dataset for training models\n",
    "  -net NET              network type for training\n",
    "  -base_kernel BASE_KERNEL\n",
    "                        base number of kernels\n",
    "  -trigger {epoch,iter}\n",
    "                        trigger type for logging\n",
    "  -interval INTERVAL    interval for logging\n",
    "  -terminal TERMINAL    terminal for training\n",
    "  -batch_size BATCH_SIZE\n",
    "                        batch_size for training\n",
    "  -lr LR                learning rate for optimization\n",
    "  -cuda CUDA            using cuda for optimization\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training with personal dataset\n",
    "### 3.1 Prepare your own dataset\n",
    "### 3.2 Run training scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Step"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
